import os
import fitz  # type: ignore
import ollama  # type: ignore
import faiss  # type: ignore
import json
from sentence_transformers import SentenceTransformer  # type: ignore
import numpy as np

# --- STEP 1: æŠ½ PDF Chunk ---
def extract_chunks_from_pdf(pdf_path, chunk_size=300):
    doc = fitz.open(pdf_path)
    full_text = ''.join([page.get_text("text") for page in doc])
    chunks = [full_text[i:i + chunk_size] for i in range(0, len(full_text), chunk_size)]
    return chunks

# --- STEP 2: å„²å­˜ / è¼‰å…¥ Chunk ---
def load_or_create_chunks(pdf_path, chunk_path, chunk_size=300):
    if os.path.exists(chunk_path):
        with open(chunk_path, "r", encoding="utf-8") as f:
            print("ğŸ”¹ å·²è¼‰å…¥å„²å­˜çš„ chunk")
            return json.load(f)
    else:
        print("ğŸ“„ æ­£åœ¨å¾ PDF å»ºç«‹ chunk...")
        chunks = extract_chunks_from_pdf(pdf_path, chunk_size)
        with open(chunk_path, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False)
        return chunks

# --- STEP 3: å»ºç«‹æˆ–è¼‰å…¥å‘é‡ç´¢å¼• ---
def load_or_create_faiss(chunks, emb_path, model_name='sentence-transformers/all-MiniLM-L6-v2'):
    model = SentenceTransformer(model_name)

    if os.path.exists(emb_path):
        print("ğŸ”¹ å·²è¼‰å…¥å„²å­˜çš„å‘é‡èˆ‡ç´¢å¼•")
        embeddings = np.load(emb_path)
    else:
        print("ğŸ” æ­£åœ¨å»ºç«‹å‘é‡åµŒå…¥...")
        embeddings = model.encode(chunks, convert_to_numpy=True)
        np.save(emb_path, embeddings)

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)

    return index, model, embeddings

# --- STEP 4: æŸ¥æ‰¾æœ€ç›¸è¿‘æ®µè½ ---
def search_similar_chunks(question, index, model, chunks, top_k=1):
    question_vector = model.encode([question], convert_to_numpy=True)
    D, I = index.search(question_vector, top_k)
    return [chunks[i] for i in I[0]]

# --- ä½¿ç”¨è€…å‡è³‡æ–™ï¼ˆå¯æ›¿æ›ç‚º JSON è®€å–ï¼‰ ---
employee_info = {
    "name": "å“¡å·¥A",
    "employee_id": "00048377",
    "personal_leave_taken": 5,
    "personal_leave_limit": 14,
    "annual_leave_taken": 3,
    "annual_leave_total": 10,
    "marriage_leave_remaining": 2
}

# --- è¨­å®šè·¯å¾‘ ---
pdf_path = "D:\\HRå°åŠ©æ‰‹_POC\\06-06ã€è«‹å‡ã€å‡åˆ¥è¦å®šã€20170101é™„ä»¶äº”(TW).pdf"
chunk_path = "chunks.json"
emb_path = "embeddings.npy"

# --- è¼‰å…¥ / å»ºç«‹ chunks + embedding + index ---
chunks = load_or_create_chunks(pdf_path, chunk_path)
index, embed_model, embeddings = load_or_create_faiss(chunks, emb_path)

# --- ä½¿ç”¨è€…è¼¸å…¥å•é¡Œ ---
user_question = input("è«‹è¼¸å…¥ä½ çš„å•é¡Œï¼š")
relevant_chunks = search_similar_chunks(user_question, index, embed_model, chunks)

# --- å»ºç«‹ Promptï¼ˆåªç”¨ top 1 æ®µï¼‰ ---
system_prompt = f"""
ä½ æ˜¯å…¬å¸çš„ HR AI åŠ©ç†ï¼Œè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡æ ¹æ“šä»¥ä¸‹ã€Œè«‹å‡æ”¿ç­–ã€èˆ‡ã€Œå“¡å·¥è³‡æ–™ã€å›ç­”å•é¡Œï¼Œä¸¦ä¸”ä»¥ç°¡æ½”æ¸…æ¥šçš„æ–¹å¼å›è¦†ã€‚

ã€è«‹å‡æ”¿ç­–ã€‘:
{relevant_chunks[0]}

ã€å“¡å·¥è«‹å‡ç´€éŒ„ã€‘:
å§“åï¼š{employee_info['name']}
äº‹å‡ï¼š{employee_info['personal_leave_taken']} / {employee_info['personal_leave_limit']} å¤©
ç‰¹ä¼‘ï¼š{employee_info['annual_leave_taken']} / {employee_info['annual_leave_total']} å¤©
å©šå‡å‰©é¤˜ï¼š{employee_info['marriage_leave_remaining']} å¤©
"""

# --- å‘¼å« LLM ---
response = ollama.chat(model='mistral', messages=[
    {'role': 'system', 'content': system_prompt},
    {'role': 'user', 'content': user_question}
])

# --- è¼¸å‡ºå›ç­” ---
print("\nğŸ§  HR AI å›è¦†ï¼š")
print(response['message']['content'])
